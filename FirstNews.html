<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="NewsPagesStyle.css">
    <!--رابط الخط-->
    <link href="https://fonts.googleapis.com/css2?family=El+Messiri:wght@500;600&display=swap" rel="stylesheet">
    <script type="text/javaScript" src="script.js"></script>
    <title>الأخبار</title>
</head>
<body>
    <header>
        <img onclick="myfunctionhome()" src="image/title.png" id="basha"/> <!--باشمبرمج لحاله--> 
        <h1 id="firstheader-news">COMPUTER SCIENTISTS FIND A KEY RESEACH ALGORITHM’S LIMITS</h1>
        <img src="image/header-image-1.png" id="image1" />
        <p id="news1p">The most widely used technique for optimizing values of a math function turns out to be a fundamentally difficult computational problem.
            Many aspects of modern</br>applied research rely on a crucial algorithm called gradient descent.This is a procedure generally used for finding the largest or smallest values of a particular</br>mathematical function—a process known as optimizing the function. It can be used to calculate anything from the most profitable way to manufacture a product</br> to the best way to assign shifts to workers.
            Yet despite this widespread usefulness, researchers have never fully understood which situations the algorithm</br> struggles with most.</br> Now, new work explains it, establishing that gradient descent, at heart, tackles a fundamentally difficult computational problem.The new result places limits on the</br> type of performance researchers can expect from the technique in particular applications.
            </br></br>Computing:
            </br>You can imagine a function as a landscape, where the elevation of the land is equal to the value of the function (the “profit”) at that particular spot. Gradient descent</br>
            searches for the function’s local minimum by looking for the direction of steepest ascent at a given location and searching downhill away from it.The slope of the</br> 
            landscape is called the gradient, hence the name gradient descent.
            Gradient descent is an essential tool of modern applied research, but there are many common</br> problems for which it does not work well. But before this research, there was no comprehensive understanding of exactly what makes gradient descent struggle and</br>when—questions another area of computer science known as computational complexity theory helped to answer.
            </br>
            </br>Computational complexity is the study of the resources, often computation time, required to solve or verify the solutions to different computing problems.</br>Researchers sort problems into different classes, with all problems in the same class sharing some fundamental computational characteristics.
            To take an example--</br>one that’s relevant to the new paper—imagine a town where there are more people than houses and everyone lives in a house. You’re given a phone book with the</br>names and addresses of everyone in town, and you’re asked to find two people who live in the same house. You know you can find an answer, because there are more</br>people than houses, but it may take some looking (especially if they don’t share a last name).
            This question belongs to a complexity class called TFNP, short for “total</br>function nondeterministic polynomial.” It is the collection of all computational problems that are guaranteed to have solutions and whose solutions can be checked</br>for correctness quickly. The researchers focused on the intersection of two subsets of problems within TFNP.
            The first subset is called PLS (polynomial local search).</br>This is a collection of problems that involve finding the minimum or maximum value of a function in a particular region. These problems are guaranteed to have</br>answers that can be found through relatively straightforward reasoning.
            One problem that falls into the PLS category is the task of planning a route that allows you</br>to visit some fixed number of cities with the shortest travel distance possible given that you can only ever change the trip by switching the order of any pair </br>of consecutive cities in the tour. It’s easy to calculate the length of any proposed route and, with a limit on the ways you can tweak the itinerary, it’s easy to see which</br>changes shorten the trip. You’re guaranteed to eventually find a route you can’t improve with an acceptable move—a local minimum.
            The second subset of problems</br>is PPAD (polynomial parity arguments on directed graphs). These problems have solutions that emerge from a more complicated process called Brouwer’s fixed</br>point theorem. The theorem says that for any continuous function, there is guaranteed to be one point that the function leaves unchanged—a fixed point, as it’s</br>known. This is true in daily life. If you stir a glass of water, the theorem guarantees that there absolutely must be one particle of water that will end up in the same</br>place it started from.
            </br></br>The intersection of the PLS and PPAD classes itself forms a class of problems known as PLS int PPAD. It contains many natural problems relevant to complexity </br>researchers. However, until now, researchers were unable to find a natural problem that’s complete for PLS int PPAD—meaning that it is an example of the hardest</br>possible problems that fall within the class.
            Prior to this paper, the only known PLS int PPAD-complete problem was a rather artificial construction—</br>a problem sometimes called “Either-Solution.” This problem glued together a complete problem from PLS and a complete problem from PPAD, forming something</br>a researcher would be unlikely to encounter outside this context. In the new paper, the researchers proved that gradient descent is as hard as Either-Solution,</br>making gradient descent itself PLS int PPAD-complete.
            </br></br>“[The nature of computation] is something that we as a species should try to understand deeply in all of its many forms. And I think that should be reason enough to </br>be excited about this result,” said Tim Roughgarden of Columbia University.
            None of this means that gradient descent will always struggle. In fact, it’s just as fast</br>and effective as ever for most uses.
            “[It] puts the brakes on what [they] can possibly shoot for,” said Daskalakis.

            They must, and in practice do, compromise</br>somewhere. They either accept a less precise solution, limit themselves to slightly easier problems, or find a way to manage an unwieldy runtime.</br>
            But this is not to say a fast algorithm for gradient descent doesn’t exist. It might. But the result does mean that any such algorithm would immediately imply</br>the existence of fast algorithms for all other problems in PLS int PPAD—a much higher bar than merely finding a fast algorithm for gradient descent itself.
            “Many</br>problems that may be some advance in mathematics could crack,” said Daskalakis. “That’s why we like to have a very natural problem like gradient descent that</br>captures the complexity of the whole intersection.”
            Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the</br>Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and</br>life sciences.
        </p>
    </body>
</html>